{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b897037",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da149e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file if it exists\n",
    "\n",
    "# Use a GitHub token from an environment variable for authentication (if available)\n",
    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', '')  # or set your token here as a string\n",
    "\n",
    "# Prepare a requests session for GitHub API calls\n",
    "session = requests.Session()\n",
    "if GITHUB_TOKEN:\n",
    "    session.headers.update({\"Authorization\": f\"token {GITHUB_TOKEN}\"})\n",
    "# GitHub API recommended headers\n",
    "session.headers.update({\"Accept\": \"application/vnd.github+json\"})\n",
    "session.headers.update({\"X-GitHub-Api-Version\": \"2022-11-28\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target languages for repository primary language\n",
    "languages = [\"TypeScript\", \"Python\", \"Go\", \"Rust\", \"C\", \"C++\"]  # \"C/C++\" means both C and C++\n",
    "# Construct the search query for these languages (multiple language qualifiers act as an OR in search)\n",
    "language_query = \" \".join(f\"language:{lang}\" for lang in languages)\n",
    "\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "per_page = 20\n",
    "\n",
    "popular_repos = []  # to collect repository data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65afa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get all pages of results for a GitHub API endpoint that may paginate\n",
    "def get_all_pages(url, params=None):\n",
    "    \"\"\"Fetch all pages of results from a GitHub API GET endpoint (combines 'Link' header pagination).\"\"\"\n",
    "    results = []\n",
    "    while url:\n",
    "        response = session.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        # Some endpoints return a list directly, others (like search) return dict with 'items'\n",
    "        if isinstance(data, dict) and \"items\" in data:\n",
    "            items = data[\"items\"]\n",
    "        else:\n",
    "            items = data\n",
    "        results.extend(items)\n",
    "        # Check for pagination in the \"Link\" header\n",
    "        link_header = response.headers.get(\"Link\", \"\")\n",
    "        next_url = None\n",
    "        if link_header:\n",
    "            # Parse the Link header to find the URL for next page\n",
    "            parts = link_header.split(\",\")\n",
    "            for part in parts:\n",
    "                if 'rel=\"next\"' in part:\n",
    "                    # Extract the next page URL from angle brackets <>\n",
    "                    start = part.find(\"<\") + 1\n",
    "                    end = part.find(\">\")\n",
    "                    next_url = part[start:end]\n",
    "                    break\n",
    "        url = next_url  # if next_url is None, loop will exit\n",
    "        params = None   # Only pass params on first request\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded ignore list - repositories to skip in the analysis\n",
    "ignore_list = {\n",
    "    \"freeCodeCamp/freeCodeCamp\",\n",
    "    \"EbookFoundation/free-programming-books\", \n",
    "    \"public-apis/public-apis\",\n",
    "    \"kamranahmedse/developer-roadmap\",\n",
    "    \"donnemartin/system-design-primer\",\n",
    "    \"vinta/awesome-python\"\n",
    "}\n",
    "\n",
    "selected_repos = []  # repositories with >=3 contributors\n",
    "contributors_dict = {}  # to store contributor login sets for each repo\n",
    "\n",
    "print(\"Starting repository collection...\")\n",
    "print(f\"Target: 20 repositories with >1 contributors\")\n",
    "print(f\"Ignoring {len(ignore_list)} repositories from ignore list\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Loop through search result pages until we have at least 20 qualified repositories\n",
    "page = 1\n",
    "while len(selected_repos) < 20:\n",
    "    print(f\"\\nðŸ“„ Fetching page {page} of search results...\")\n",
    "    params = {\"q\": language_query, \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": per_page, \"page\": page}\n",
    "    response = session.get(search_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_data = response.json()\n",
    "    repo_items = search_data.get(\"items\", [])\n",
    "    if not repo_items:\n",
    "        print(\"âŒ No more results found\")\n",
    "        break  # no more results\n",
    "    \n",
    "    print(f\"Found {len(repo_items)} repositories on this page\")\n",
    "    \n",
    "    for i, repo in enumerate(repo_items, 1):\n",
    "        print(f\"\\nðŸ” [{i}/{len(repo_items)}] Checking: {repo['full_name']} (â­ {repo['stargazers_count']}, {repo.get('language')})\")\n",
    "        \n",
    "        # Skip repositories in the ignore list\n",
    "        if repo[\"full_name\"] in ignore_list:\n",
    "            print(f\"â­ï¸  Skipping ignored repository: {repo['full_name']}\")\n",
    "            continue\n",
    "            \n",
    "        # Fetch all contributors for this repo (contributors endpoint is publicly accessible)\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "        contributors_url = f\"https://api.github.com/repos/{owner}/{repo_name}/contributors\"\n",
    "        \n",
    "        print(f\"   Fetching contributors...\")\n",
    "        try:\n",
    "            contributors = get_all_pages(contributors_url, params={\"per_page\": 100})\n",
    "            print(f\"   Found {len(contributors)} contributors\")\n",
    "            \n",
    "            if len(contributors) > 1:  # more than 1 contributor\n",
    "                selected_repos.append(repo)\n",
    "                # Store the set of contributor usernames for later checking comments\n",
    "                contributors_set = {contributor[\"login\"] for contributor in contributors}\n",
    "                contributors_dict[repo[\"full_name\"]] = contributors_set\n",
    "                print(f\"âœ… Added to selection ({len(selected_repos)}/20): {repo['full_name']}\")\n",
    "            else:\n",
    "                print(f\"âŒ Skipping: Only {len(contributors)} contributor(s)\")\n",
    "                \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Skip repositories where we can't access contributors (e.g., private repos)\n",
    "            print(f\"âŒ Skipping {repo['full_name']}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        if len(selected_repos) >= 20:\n",
    "            print(f\"\\nðŸŽ‰ Target reached! Found {len(selected_repos)} repositories\")\n",
    "            break\n",
    "    page += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "# Check how many repositories we collected\n",
    "print(f\"âœ… Collected {len(selected_repos)} repositories with >=2 contributors.\")\n",
    "print(\"\\nSelected repositories:\")\n",
    "for repo in selected_repos:\n",
    "    print(f\"- {repo['full_name']} (Primary language: {repo.get('language')}, â­ {repo['stargazers_count']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare lists to collect lead times\n",
    "lead_times_groupA = []  # issues with >=2 maintainer commenters\n",
    "lead_times_groupB = []  # issues with <=1 maintainer commenter\n",
    "\n",
    "analysed_repos = selected_repos[:] # Slightly easier to test things out by manually adding a number to slice the list\n",
    "\n",
    "print(\"Starting issue analysis for lead time calculation...\")\n",
    "print(f\"Analyzing {len(analysed_repos)} repositories\")\n",
    "print(\"Group A: Issues with >=2 maintainer commenters\")\n",
    "print(\"Group B: Issues with <=1 maintainer commenter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for repo_idx, repo in enumerate(analysed_repos, 1):\n",
    "    full_name = repo[\"full_name\"]\n",
    "    owner = repo[\"owner\"][\"login\"]\n",
    "    repo_name = repo[\"name\"]\n",
    "\n",
    "    print(f\"\\nðŸ“ [{repo_idx}/{len(analysed_repos)}] Processing repository: {full_name}\")\n",
    "    print(f\"   Language: {repo.get('language')}, Stars: â­ {repo['stargazers_count']}\")\n",
    "    \n",
    "    # Use GitHub Search API to fetch only issues (not pull requests) that are closed\n",
    "    search_issues_url = \"https://api.github.com/search/issues\"\n",
    "    search_query = f\"repo:{full_name} type:issue state:closed comments:>0\"\n",
    "    # Add involves:<username> for every contributor in this repo, if available, since we're only interested in issues where maintainers commented\n",
    "    # repo_contributors = contributors_dict.get(full_name, set())\n",
    "    # if repo_contributors:\n",
    "    #     involves_clauses = [f\"involves:{username}\" for username in repo_contributors]\n",
    "    #     search_query = search_query + \" \" + \" \".join(involves_clauses)\n",
    "    print(f\"   ðŸ” Fetching closed issues only (excluding pull requests)...\")\n",
    "    \n",
    "    # Get all closed issues using search API (this excludes PRs automatically)\n",
    "    search_response = get_all_pages(search_issues_url, params={\"q\": search_query, \"per_page\": 100, \"advanced_search\": \"true\"})\n",
    "\n",
    "    # Unfortunately, GitHub only ever returns a maximum of 1000 results for search queries\n",
    "\n",
    "    # Extract the actual issues from search results\n",
    "    issues = search_response if isinstance(search_response, list) else []\n",
    "    \n",
    "    print(f\"   ðŸ“ Found {len(issues)} closed issues (pull requests excluded)\")\n",
    "    \n",
    "    maintainers_set = contributors_dict.get(full_name, set())\n",
    "    print(f\"   ðŸ‘¥ Repository has {len(maintainers_set)} contributors\")\n",
    "    \n",
    "    # Counters for this repository\n",
    "    repo_issues_processed = 0\n",
    "    repo_groupA_added = 0\n",
    "    repo_groupB_added = 0\n",
    "    \n",
    "    # Process each issue\n",
    "    for issue_idx, issue in enumerate(issues, 1):\n",
    "        if issue_idx % 200 == 0 or issue_idx == len(issues):\n",
    "            print(f\"      Processing item {issue_idx}/{len(issues)}...\")\n",
    "            \n",
    "        # No need to check for pull requests anymore since we're using type:issue filter\n",
    "        \n",
    "        # Calculate lead time from creation to closing\n",
    "        created = issue.get(\"created_at\")\n",
    "        closed = issue.get(\"closed_at\")\n",
    "        if not closed or not created:\n",
    "            continue  # skip if missing timestamps (shouldn't happen for closed issues)\n",
    "            \n",
    "        # Parse timestamps to datetime objects\n",
    "        # Using built-in datetime parsing; assuming timestamps are in ISO 8601 (e.g. \"2021-07-19T09:23:43Z\")\n",
    "        from datetime import datetime\n",
    "        created_dt = datetime.fromisoformat(created.replace(\"Z\", \"+00:00\"))\n",
    "        closed_dt = datetime.fromisoformat(closed.replace(\"Z\", \"+00:00\"))\n",
    "        \n",
    "        # Lead time in days (as float)\n",
    "        lead_time_days = (closed_dt - created_dt).total_seconds() / (60*60*24)\n",
    "        \n",
    "        # Determine how many unique maintainers commented on this issue\n",
    "        maint_commenters = 0\n",
    "        if issue.get(\"comments\", 0) == 0:\n",
    "            # No comments at all\n",
    "            maint_commenters = 0\n",
    "        elif issue.get(\"comments\", 0) == 1:\n",
    "            # Only one comment; check if it was by a maintainer\n",
    "            comments_url = issue[\"comments_url\"]\n",
    "            comment_data = get_all_pages(comments_url)\n",
    "            # comment_data could be a list of one comment (if we have permission), or empty if not accessible\n",
    "            if isinstance(comment_data, list) and comment_data:\n",
    "                commenter_login = comment_data[0][\"user\"][\"login\"]\n",
    "                maint_commenters = 1 if commenter_login in maintainers_set else 0\n",
    "            else:\n",
    "                # If comment not accessible, assume 0 maintainers for safety\n",
    "                maint_commenters = 0\n",
    "        else:\n",
    "            # Multiple comments, fetch all comments and count unique maintainers among commenters\n",
    "            comments_url = issue[\"comments_url\"]\n",
    "            all_comments = get_all_pages(comments_url)\n",
    "            # Get unique logins of commenters who are maintainers\n",
    "            maintainer_commenters = {c[\"user\"][\"login\"] for c in all_comments if c[\"user\"][\"login\"] in maintainers_set}\n",
    "            maint_commenters = len(maintainer_commenters)\n",
    "            \n",
    "        # Categorize issue based on maintainer commenter count\n",
    "        if maint_commenters >= 2:\n",
    "            lead_times_groupA.append(lead_time_days)\n",
    "            repo_groupA_added += 1\n",
    "        else:\n",
    "            lead_times_groupB.append(lead_time_days)\n",
    "            repo_groupB_added += 1\n",
    "            \n",
    "        repo_issues_processed += 1\n",
    "\n",
    "    # Repository summary\n",
    "    print(f\"   âœ… Repository complete:\")\n",
    "    print(f\"      â€¢ {repo_issues_processed} issues processed (PRs automatically excluded)\")\n",
    "    print(f\"      â€¢ {repo_groupA_added} â†’ Group A (>=2 maintainer commenters)\")\n",
    "    print(f\"      â€¢ {repo_groupB_added} â†’ Group B (<=1 maintainer commenter)\")\n",
    "    print(f\"   ðŸ“Š Running totals: Group A={len(lead_times_groupA)}, Group B={len(lead_times_groupB)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸŽ‰ Issue analysis complete!\")\n",
    "print(f\"ðŸ“Š Final Results:\")\n",
    "print(f\"   â€¢ Total repositories analyzed: {len(analysed_repos)}\")\n",
    "print(f\"   â€¢ Group A (>=2 maintainer commenters): {len(lead_times_groupA)} issues\")\n",
    "print(f\"   â€¢ Group B (<=1 maintainer commenter): {len(lead_times_groupB)} issues\")\n",
    "print(f\"   â€¢ Total issues analyzed: {len(lead_times_groupA) + len(lead_times_groupB)}\")\n",
    "\n",
    "if lead_times_groupA:\n",
    "    print(f\"   â€¢ Group A average lead time: {sum(lead_times_groupA)/len(lead_times_groupA):.1f} days\")\n",
    "if lead_times_groupB:\n",
    "    print(f\"   â€¢ Group B average lead time: {sum(lead_times_groupB)/len(lead_times_groupB):.1f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58714b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median lead times for both groups\n",
    "median_A = np.median(lead_times_groupA) if lead_times_groupA else float('nan')\n",
    "median_B = np.median(lead_times_groupB) if lead_times_groupB else float('nan')\n",
    "\n",
    "print(f\"\\nMedian lead time for issues with >=2 maintainer commenters (Group A): {median_A:.2f} days\")\n",
    "print(f\"Median lead time for issues with <=1 maintainer commenter (Group B): {median_B:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a99d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers using IQR method for better visualization\n",
    "def remove_outliers_iqr(data, multiplier=1.5):\n",
    "    \"\"\"Remove outliers using the Interquartile Range (IQR) method\"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    \n",
    "    filtered_data = [x for x in data if lower_bound <= x <= upper_bound]\n",
    "    outliers_removed = len(data) - len(filtered_data)\n",
    "    \n",
    "    return filtered_data, outliers_removed, lower_bound, upper_bound\n",
    "\n",
    "# Filter outliers from both groups\n",
    "lead_times_groupA_filtered, outliers_A, lower_A, upper_A = remove_outliers_iqr(lead_times_groupA)\n",
    "lead_times_groupB_filtered, outliers_B, lower_B, upper_B = remove_outliers_iqr(lead_times_groupB)\n",
    "\n",
    "print(\"Outlier filtering results:\")\n",
    "print(f\"Group A: {len(lead_times_groupA)} â†’ {len(lead_times_groupA_filtered)} issues ({outliers_A} outliers removed)\")\n",
    "print(f\"  Range after filtering: {lower_A:.1f} to {upper_A:.1f} days\")\n",
    "print(f\"Group B: {len(lead_times_groupB)} â†’ {len(lead_times_groupB_filtered)} issues ({outliers_B} outliers removed)\")\n",
    "print(f\"  Range after filtering: {lower_B:.1f} to {upper_B:.1f} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3243cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate appropriate X-axis limit based on filtered data\n",
    "max_lead_time = max(\n",
    "    max(lead_times_groupA_filtered) if lead_times_groupA_filtered else 0,\n",
    "    max(lead_times_groupB_filtered) if lead_times_groupB_filtered else 0\n",
    ")\n",
    "x_limit = min(max_lead_time * 1.1, 365)  # Cap at 1 year for readability\n",
    "\n",
    "print(f\"\\nChart X-axis limit set to: {x_limit:.0f} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f562d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot distributions of lead times for both groups (filtered)\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Group A distribution plot\n",
    "if lead_times_groupA_filtered:\n",
    "    sns.histplot(lead_times_groupA_filtered, kde=True, ax=axes[0], color='skyblue', bins=30)\n",
    "    axes[0].axvline(np.median(lead_times_groupA_filtered), color='navy', linestyle='--', alpha=0.7, \n",
    "                   label=f'Median: {np.median(lead_times_groupA_filtered):.1f} days')\n",
    "    axes[0].legend()\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No data after filtering', transform=axes[0].transAxes, ha='center')\n",
    "\n",
    "axes[0].set_title(f'Group A: >=2 Maintainer Comments\\n({len(lead_times_groupA_filtered)} issues, {outliers_A} outliers removed)')\n",
    "axes[0].set_xlabel('Lead Time (days)')\n",
    "axes[0].set_ylabel('Issue Count')\n",
    "axes[0].set_xlim(0, x_limit)\n",
    "\n",
    "# Group B distribution plot\n",
    "if lead_times_groupB_filtered:\n",
    "    sns.histplot(lead_times_groupB_filtered, kde=True, ax=axes[1], color='salmon', bins=30)\n",
    "    axes[1].axvline(np.median(lead_times_groupB_filtered), color='darkred', linestyle='--', alpha=0.7,\n",
    "                   label=f'Median: {np.median(lead_times_groupB_filtered):.1f} days')\n",
    "    axes[1].legend()\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No data after filtering', transform=axes[1].transAxes, ha='center')\n",
    "\n",
    "axes[1].set_title(f'Group B: <=1 Maintainer Comment\\n({len(lead_times_groupB_filtered)} issues, {outliers_B} outliers removed)')\n",
    "axes[1].set_xlabel('Lead Time (days)')\n",
    "axes[1].set_ylabel('Issue Count')\n",
    "axes[1].set_xlim(0, x_limit)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print summary statistics for filtered data\n",
    "print(f\"\\nSummary statistics (after outlier removal):\")\n",
    "if lead_times_groupA_filtered:\n",
    "    print(f\"Group A (>=2 maintainer comments):\")\n",
    "    print(f\"  â€¢ Count: {len(lead_times_groupA_filtered)} issues\")\n",
    "    print(f\"  â€¢ Median: {np.median(lead_times_groupA_filtered):.1f} days\")\n",
    "    print(f\"  â€¢ Mean: {np.mean(lead_times_groupA_filtered):.1f} days\")\n",
    "    print(f\"  â€¢ Range: {min(lead_times_groupA_filtered):.1f} - {max(lead_times_groupA_filtered):.1f} days\")\n",
    "\n",
    "if lead_times_groupB_filtered:\n",
    "    print(f\"Group B (<=1 maintainer comment):\")\n",
    "    print(f\"  â€¢ Count: {len(lead_times_groupB_filtered)} issues\")\n",
    "    print(f\"  â€¢ Median: {np.median(lead_times_groupB_filtered):.1f} days\")\n",
    "    print(f\"  â€¢ Mean: {np.mean(lead_times_groupB_filtered):.1f} days\")\n",
    "    print(f\"  â€¢ Range: {min(lead_times_groupB_filtered):.1f} - {max(lead_times_groupB_filtered):.1f} days\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
